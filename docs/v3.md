# Welcome to PowerQE V3 Document

## Create environment

> Chinese users may first update mirrors:
>
> - Conda: https://mirrors.tuna.tsinghua.edu.cn/help/anaconda
> - pip: https://mirrors.tuna.tsinghua.edu.cn/help/pypi

First, create a PowerQE environment:

```bash
conda env create -f environment.yml  # create the powerqe env
conda activate powerqe
```

Then, install MMEditing following "mmediting/docs/en/install.md":

```bash
conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch

pip3 install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html
# or
#pip3 install openmim
#mim install mmcv-full==1.7.0

cd mmediting
pip3 install -e .

# verify
cd ~
python -c "import mmedit; print(mmedit.__version__)"
```

Finally, install requirements of PowerQE:

```bash
cd <powerqe-root>
pip3 install -r requirements.txt
```

## Compress image and video

### BPG

[Better Portable Graphics (BPG)](https://bellard.org/bpg) is a image format based on the intra-frame encoding of the High Efficiency Video Coding (HEVC) standard.

Please refer to the official site and the [GitHub mirror](https://github.com/mirrorer/libbpg/blob/master/README) for instructions. The following is my personal experience of using libbpg v0.9.8.

#### Build

Clone:

```bash
cd data
git clone --depth=1 https://github.com/mirrorer/libbpg.git libbpg
```

Modify "libbpg/Makefile":

- Comment `USE_X265=y` and uncomment `USE_JCTVC=y`: We want to use JCTVC instead of x265.
- Comment `USE_BPGVIEW=y`: We do not need BPGView.

Build:

```bash
cd data/libbpg
make clean
make
```

There may be errors during the build. We need to install the required dependencies according to the error messages. For examples,

```bash
sudo apt-get install libpng-dev
sudo apt-get install libjpeg-dev

# build again
make clean
make
```

#### Example: Compress the DIV2K dataset

> BPG basic usage:
>
> ```bash
> bpgenc [-q quality] -o bpg-path src-path  # encode .png to .bpg
> bpgdec -o tar-path bpg-path  # decode .bpg to .png
> ```
>
> Check `bpgenc -h` and `bpgdec -h` for detailed usage.

Write a Bash script to run the following script, or paste it directly into the terminal:

```bash
BPGENC_PATH="data/libbpg/bpgenc"
BPGDEC_PATH="data/libbpg/bpgdec"
QP=37

# training set
SRC_DIR="data/div2k/train/gt"
TMP_DIR="tmp/bpg/div2k/train"
TAR_DIR="data/div2k/train/lq"
mkdir -p $TMP_DIR
mkdir -p $TAR_DIR
for idx in `seq -f '%04g' 1 800`; do
  echo $idx
  $BPGENC_PATH -o $TMP_DIR/$idx.bpg -q $QP $SRC_DIR/$idx.png
  $BPGDEC_PATH -o $TAR_DIR/$idx.png -b 8 $TMP_DIR/$idx.bpg
done

# valid set
SRC_DIR="data/div2k/valid/gt"
TMP_DIR="tmp/bpg/div2k/valid"
TAR_DIR="data/div2k/valid/lq"
mkdir -p $TMP_DIR
mkdir -p $TAR_DIR
for idx in `seq -f '%04g' 801 900`; do
  echo $idx
  $BPGENC_PATH -o $TMP_DIR/$idx.bpg -q $QP $SRC_DIR/$idx.png
  $BPGDEC_PATH -o $TAR_DIR/$idx.png -b 8 $TMP_DIR/$idx.bpg
done
```

To speed up the process, we can also use Python multiprocessing:

```bash
conda activate powerqe &&\
 python tools/data/compress_img.py --dataset div2k --codec bpg
```

File tree:

```txt
powerqe
`-- data
`   `-- div2k
`   `   `-- train
`   `   `   `-- gt
`   `   `   `   `-- 0001.png
`   `   `   `   `-- ...
`   `   `   `   `-- 0800.png
`   `   `   `-- lq
`   `   `-- valid
`   `       `-- gt
`   `       `   `-- 0801.png
`   `       `   `-- ...
`   `       `   `-- 0900.png
`   `       `-- lq
`   `-- libbpg
`       `-- bpgdec
`       `-- bpgenc
`-- tmp/bpg/div2k  # can be deleted after compression
    `-- train
    `   `-- 0001.bpg
    `   `-- ...
    `   `-- 0800.bpg
    `-- valid
        `-- 0801.bpg
        `-- ...
        `-- 0900.bpg
```

### HM

[HM](https://vcgit.hhi.fraunhofer.de/jvet/HM) is the reference software for High Efficiency Video Coding (HEVC).

Please refer to the official site and the document (e.g., "HM/doc/software-manual.pdf") for instructions. The following is my personal experience of using HM 18.0.

#### Build

```bash
cd data
git clone -b HM-18.0 --depth=1 https://vcgit.hhi.fraunhofer.de/jvet/HM.git hm

cd hm
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j
```

#### Example: Compress the Vimeo-90K triplet dataset

According to the HM manual, the encoder accepts videos in raw 4:2:0 planar format (Y'CbCr). So we first convert the PNG sequences into planar files.

```bash
conda activate powerqe &&\
 python tools/data/convert_img_planar.py\
 --dataset vimeo90k-triplet\
 --src data/vimeo_triplet/sequences
```

Compress planar files by HM:

```bash
conda activate powerqe &&\
 python tools/data/compress_video.py\
 --dataset vimeo90k-triplet
```

Revert the compressed planar files into PNG sequences:

```bash
conda activate powerqe &&\
 python tools/data/convert_img_planar.py\
 --dataset vimeo90k-triplet\
 --revert\
 --tar-re data/vimeo_triplet_lq
```

File tree:

```txt
powerqe
`-- data
`   `-- hm
`   `   `-- bin/TAppEncoderStatic
`   `   `-- cfg/encoder_lowdelay_P_main.cfg
`   `-- vimeo_triplet/sequences
`   `   `-- 00001
`   `   `   `-- 0001
`   `   `   `   `-- im1.png
`   `   `   `   `-- im2.png
`   `   `   `   `-- im3.png
`   `   `   `-- ...
`   `   `   `-- 1000
`   `   `-- ...
`   `   `-- 00078
`   `-- vimeo_triplet_lq
`-- tmp  # can be deleted after compression
    `-- bit/vimeo_triplet
    `   `-- 00001
    `   `   `-- 0001.bin
    `   `   `-- ...
    `   `   `-- 1000.bin
    `   `-- ...
    `   `-- 00078
    `-- comp_planar/vimeo_triplet
    `   `-- 00001
    `   `   `-- 0001.log
    `   `   `-- 0001.yuv
    `   `   `-- ...
    `   `   `-- 1000.log
    `   `   `-- 1000.yuv
    `   `-- ...
    `   `-- 00078
    `-- planar/vimeo_triplet
        `-- 00001
        `   `-- 0001.yuv
        `   `-- ...
        `   `-- 1000.yuv
        `-- ...
        `-- 00078
```

#### Why do we not use x265

[x265](https://www.x265.org) is a HEVC video encoder application library. Encoding videos using x265 can be much faster than using HM. x265 has been supported by FFmpeg with [libx265](https://trac.ffmpeg.org/wiki/Encode/H.265). As indicated by this paper[^paper-x265], the following script can generate compressed videos that closely resemble the output of HM:

```bash
ffmpeg -video_size <WIDTH>x<HEIGHT>\
 -i <INPUT>\
 -vcodec libx265\
 -qp <QP>\
 -x265-params <OPTIONS>\
 <OUTPUT>
```

for options:

```bash
<OPTIONS>=
keyint=7:min-keyint=7:no-scenecut:me=full:subme=7:bframes=0:qp=<QP>
```

For research purpose, we need to control the distortion. Therefore, it is better to use a specific configuration of a stable version of the reference software. For example, we may use the [low-delay P](https://vcgit.hhi.fraunhofer.de/jvet/HM/-/blob/fb4486d5ab5d0cd3b6a71659c7d5eb4509f2a4ce/cfg/encoder_lowdelay_P_main.cfg) configuration of HM 18.0 to compress videos. In this configuration, many hyperparameters, including the frame-level QP offset, are set. The above script does not indicate these parameters.

> The QP offset is proposed to achieve better RD performance[^qp-offset]. Some approaches such as MFQEv2[^mfqev2] take advantage of the frame-wise quality fluctuation caused by the QP offset.

## Configuration

### Crop image border before evaluation

Due to the padding of up-sampling, the error at borders is significant. PowerQE follows the common practice in MMEditing to crop image borders before evaluation.

### Test time unfolding

When using test time unfolding, patch-based evaluation is conducted to save memory. The accuracy may also drop.

## Use LMDB for faster IO

One can use LMDB to accelerate the IO. Specifically, one can store training patches/images in LMDB files.

Pros:

- Much faster training due to the loading and processing of small patches instead of big images and faster IO of LMDB.
- Lower CPU utility and GPU memory due to the loading and processing of small patches.
- All images (PNG, JPG, etc.) can be stored as PNG.

Cons:

- Higher memory.
- Extra time, computation and storage for LMDB files.
- Once the LMDB file is generated, the cropping manner is also fixed.

Take the DIV2K dataset as an example.

```bash
# cropping and making lmdb for training set

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 -src data/div2k/train/gt\
 -tmp tmp/data/div2k/train/gt_patches\
 -save data/div2k/train/gt_patches.lmdb\
 -n 16 -ps 128 -step 64

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 -src data/div2k/train/lq\
 -tmp tmp/data/div2k/train/lq_patches\
 -save data/div2k/train/lq_patches.lmdb\
 -n 16 -ps 128 -step 64

# no cropping for test set

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 -no-patch\
 -src data/div2k/valid/gt\
 -save data/div2k/valid/gt.lmdb\
 -n 16

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 -no-patch\
 -src data/div2k/valid/lq\
 -save data/div2k/valid/lq.lmdb\
 -n 16
```

For the configuration file with LMDB loading, see "configs/arcnn_div2k_lmdb.py".

## Use pre-commit hook for auto code check

PowerQE follows [MMCV](https://github.com/open-mmlab/mmcv/blob/master/CONTRIBUTING.md) and MMEditing to support the pre-commit hook. The configuration file is inherited from mmediting/.pre-commit-config.yaml. Installation:

```bash
conda activate powerqe
pip install -U pre-commit
pre-commit install
```

On every commit, linters and formatter will be enforced. You can also run hooks manually:

```bash
pre-commit run --all
```

## Others

### Key frames

Some approaches such as MFQEv2[^mfqev2] take advantage of the frame-wise quality fluctuation caused by the QP offset. We consider key frames as those with the lowest QP locally. Note that we do not consider PSNR since it also correlates to the content, while QP only correlates to the compression. This strategy is effective for low-delay P configuration, which turns off the rate control and fixes QP.

You can find `QP`, `IntraQPOffset`, `QPoffset`, `QPOffsetModelOff`, and `QPOffsetModelScale` in the configuration, i.e., "data/hm/cfg/encoder_lowdelay_P_main.cfg". In addition, you can find the frame QPs in log files. How do `QP`, `IntraQPOffset`, `QPoffset`, `QPOffsetModelOff`, and `QPOffsetModelScale` determine the final frame QPs?

- POC 0 (I-SLICE): `QP + IntraQPOffset`
- POC 1, 2, ... (P-SLICE): `QP + QPoffset + QPOffsetModelOff + QPOffsetModelScale * QP`

For example, the `QP`, `IntraQPOffset`, `QPoffset`, `QPOffsetModelOff`, and `QPOffsetModelScale` are `37`, `-1`, `[5, 4, 5, 4, 5, 4, 5, 1]`, `[-6.5, ..., -6.5, 0]`, and `[0.2590, ..., 0.2590, 0]`, respectively. Then, QP values for POC 0 to 8 are `36`, `45`, `44`, `45`, `44`, `45`, `44`, `45`, and `38`, respectively. Since a lower QP correlates to higher compression quality, POC 0, 2, 4, 6, and 8 are key frames.

> The configuration can vary among different HM versions. In HM 16.5, QP offsets are set to `[5,4,5,1]` (GOPSize: 4).
>
> You can also find a configuration with GOPSize being 4 for HM 18.0 at "data/hm/cfg/misc/encoder_lowdelay_P_main_GOP4.cfg".

### Same items between PowerQE and MMEditing

We take the `Compose` in "powerqe/datasets/pipelines/compose.py" as an example.

When constructing the pipeline for a dataset, the dataset (`BaseDataset` in fact) refers to `Compose`. Then, `Compose` refers to `..registry` for `PIPELINES`.

PowerQE has its own transforms such as `PairedCenterCrop`. As a result, PowerQE has to define a new `Compose`, which refers to its own `..registry`.

Note that the `Compose` in PowerQE will not be registered into `PIPELINES`.

[^paper-x265]: *Leveraging Bitstream Metadata for Fast and Accurate Video Compression Correction*, 2022.

[^qp-offset]: Proposal [JCTVC-X0038](http://phenix.it-sudparis.eu/jct/doc_end_user/current_document.php?id=10496).

[^mfqev2]: *MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video*, 2019.
