# Welcome to PowerQE V3 Document

## Create environment

> Chinese users may first update mirrors:
>
> - Conda: https://mirrors.tuna.tsinghua.edu.cn/help/anaconda
> - pip: https://mirrors.tuna.tsinghua.edu.cn/help/pypi

First, create a PowerQE environment:

```bash
conda env create -f environment.yml  # create the powerqe env
conda activate powerqe
```

Then, install MMEditing following `mmediting/docs/en/install.md`. My code:

```bash
pip3 install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117

pip3 install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu117/torch1.13/index.html
# or
#pip3 install openmim
#mim install mmcv-full==1.7.0

pip3 install -e mmediting

# verify
#cd ~
#python -c "import mmedit; print(mmedit.__version__)"
```

## Prepare datasets

### DIVerse 2K (DIV2K)

https://data.vision.ee.ethz.ch/cvl/DIV2K/

```txt
div2k
`-- train
`   `-- 0001.png
`   `-- ...
`   `-- 0800.png
`-- valid
`   `-- 0801.png
`   `-- ...
`   `-- 0900.png
```

### Vimeo-90K

http://toflow.csail.mit.edu

#### Triplet dataset

To shorten the validation time during training, we select 42 sequences (matched by `00001/\*`) in `tri_testlist.txt` (3782 sequences in total) for validation and construct `tri_validlist.txt`.

```txt
vimeo_triplet
`-- tri_trainlist.txt
`-- tri_validlist.txt
`-- tri_testlist.txt
`-- sequences
`   `-- 00001
`   `   `-- 0001
`   `   `   `-- im1.png
`   `   `   `-- im2.png
`   `   `   `-- im3.png
`   `   `-- ...
`   `   `-- 1000
`   `-- ...
`   `-- 00078
```

#### Septuplet dataset

To shorten the validation time during training, we select 58 sequences (matched by `00001/\*`) in `sep_testlist.txt` (7824 sequences in total) for validation and construct `sep_validlist.txt`.

```txt
vimeo_septuplet
`-- sep_trainlist.txt
`-- sep_validlist.txt
`-- sep_testlist.txt
`-- sequences
`   `-- 00001
`   `   `-- 0001
`   `   `   `-- im1.png
`   `   `   `-- ...
`   `   `   `-- im7.png
`   `   `-- ...
`   `   `-- 1000
`   `-- ...
`   `-- 00096
```

## Compress image and video

### BPG

[Better Portable Graphics (BPG)](https://bellard.org/bpg) is a image format based on the intra-frame encoding of the High Efficiency Video Coding (HEVC) standard.

Please refer to the official site and the [GitHub mirror](https://github.com/mirrorer/libbpg/blob/master/README) for instructions. The following is my personal experience of using libbpg v0.9.8.

#### Build

Clone:

```bash
cd data
git clone --depth=1 https://github.com/mirrorer/libbpg.git libbpg
```

Modify `libbpg/Makefile`:

- Comment `USE_X265=y` and uncomment `USE_JCTVC=y`: We want to use JCTVC instead of x265.
- Comment `USE_BPGVIEW=y`: We do not need BPGView.

Build:

```bash
cd data/libbpg
make clean
make
```

There may be errors during the build. We need to install the required dependencies according to the error messages. For examples,

```bash
sudo apt-get install libpng-dev
sudo apt-get install libjpeg-dev

# build again
make clean
make
```

#### Example: Compress the DIV2K dataset

> BPG basic usage:
>
> ```bash
> bpgenc [-q quality] -o bpg-path src-path  # encode .png to .bpg
> bpgdec -o tar-path bpg-path  # decode .bpg to .png
> ```
>
> Check `bpgenc -h` and `bpgdec -h` for detailed usage.

Run:

```bash
conda activate powerqe &&\
 python tools/data/compress_img.py --codec bpg --dataset div2k
```

File tree:

```txt
powerqe
`-- data
`   `-- div2k
`   `   `-- train
`   `   `   `-- 0001.png
`   `   `   `-- ...
`   `   `   `-- 0800.png
`   `   `-- valid
`   `       `-- 0801.png
`   `       `-- ...
`   `       `-- 0900.png
`   `-- div2k_lq/bpg/qp37
`   `   `-- train
`   `   `   `-- 0001.png
`   `   `   `-- ...
`   `   `   `-- 0800.png
`   `   `-- valid
`   `       `-- 0801.png
`   `       `-- ...
`   `       `-- 0900.png
`   `-- libbpg
`       `-- bpgdec
`       `-- bpgenc
`-- tmp/div2k_lq/bpg/qp37  # can be deleted after compression
    `-- train
    `   `-- 0001.bpg
    `   `-- ...
    `   `-- 0800.bpg
    `-- valid
        `-- 0801.bpg
        `-- ...
        `-- 0900.bpg
```

### HM

[HM](https://vcgit.hhi.fraunhofer.de/jvet/HM) is the reference software for High Efficiency Video Coding (HEVC).

Please refer to the official site and the document (e.g., `HM/doc/software-manual.pdf`) for instructions. The following is my personal experience of using HM 18.0.

#### Build

```bash
cd data
git clone -b HM-18.0 --depth=1 https://vcgit.hhi.fraunhofer.de/jvet/HM.git hm18.0

cd hm18.0
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j
```

#### Example: Compress the Vimeo-90K triplet dataset

```bash
conda activate powerqe &&\
 python tools/data/compress_video.py\
 --dataset vimeo90k-triplet
```

> According to the HM manual, the encoder accepts videos in raw 4:2:0 planar format (Y'CbCr). Therefore the PNG sequences are first converted into planar files.

File tree:

```txt
powerqe
`-- data
`   `-- hm18.0
`   `   `-- bin/TAppEncoderStatic
`   `   `-- cfg/encoder_lowdelay_P_main.cfg
`   `-- vimeo_triplet/sequences
`   `   `-- 00001
`   `   `   `-- 0001
`   `   `   `   `-- im1.png
`   `   `   `   `-- im2.png
`   `   `   `   `-- im3.png
`   `   `   `-- ...
`   `   `   `-- 1000
`   `   `-- ...
`   `   `-- 00078
`   `-- vimeo_septuplet_lq/hm18.0/ldp/qp37
`-- tmp  # can be deleted after compression
    `-- vimeo_septuplet_bit/hm18.0/ldp/qp37
    `   `-- 00001
    `   `   `-- 0001.bin
    `   `   `-- ...
    `   `   `-- 1000.bin
    `   `-- ...
    `   `-- 00078
    `-- vimeo_septuplet_comp_planar/hm18.0/ldp/qp37
    `   `-- 00001
    `   `   `-- 0001.log
    `   `   `-- 0001.yuv
    `   `   `-- ...
    `   `   `-- 1000.log
    `   `   `-- 1000.yuv
    `   `-- ...
    `   `-- 00078
    `-- vimeo_septuplet_planar
        `-- 00001
        `   `-- 0001.yuv
        `   `-- ...
        `   `-- 1000.yuv
        `-- ...
        `-- 00078
```

#### Why do we not use x265

[x265](https://www.x265.org) is a HEVC video encoder application library. Encoding videos using x265 can be much faster than using HM. x265 has been supported by FFmpeg with [libx265](https://trac.ffmpeg.org/wiki/Encode/H.265). As indicated by this paper[^paper-x265], the following script can generate compressed videos that closely resemble the output of HM:

```bash
ffmpeg -video_size <WIDTH>x<HEIGHT>\
 -i <INPUT>\
 -vcodec libx265\
 -qp <QP>\
 -x265-params <OPTIONS>\
 <OUTPUT>
```

for options:

```bash
<OPTIONS>=
keyint=7:min-keyint=7:no-scenecut:me=full:subme=7:bframes=0:qp=<QP>
```

For research purpose, we need to control the distortion. Therefore, it is better to use a specific configuration of a stable version of the reference software. For example, we may use the [low-delay P](https://vcgit.hhi.fraunhofer.de/jvet/HM/-/blob/fb4486d5ab5d0cd3b6a71659c7d5eb4509f2a4ce/cfg/encoder_lowdelay_P_main.cfg) configuration of HM 18.0 to compress videos. In this configuration, many hyperparameters, including the frame-level QP offset, are set. The above script does not indicate these parameters.

> The QP offset is proposed to achieve better RD performance[^qp-offset]. Some approaches such as MFQEv2[^mfqev2] take advantage of the frame-wise quality fluctuation caused by the QP offset.

## Configuration

### Obtain clean configuration without inheritance

Configuration inheritance is widely used in PowerQE to ensure consistency among configurations.

To obtain clean configuration without inheritance:

```bash
conda activate powerqe
python -c "from mmcv import Config; cfg = Config.fromfile('configs/arcnn_div2k_lmdb.py'); print(cfg.pretty_text)"
```

### Crop image border before evaluation

Due to the padding of upsampling, the error at borders is significant. PowerQE follows the common practice in MMEditing to crop image borders before evaluation.

### Test time unfolding

When using test time unfolding, patch-based evaluation is conducted to save memory. The accuracy may also drop.

## Others

### About key frames

Some approaches such as MFQEv2[^mfqev2] take advantage of the frame-wise quality fluctuation caused by the QP offset. We consider key frames as those with the lowest QP locally. Note that we do not consider PSNR since it also correlates to the content, while QP only correlates to the compression. This strategy is effective for low-delay P configuration, which turns off the rate control and fixes QP.

You can find `QP`, `IntraQPOffset`, `QPoffset`, `QPOffsetModelOff`, and `QPOffsetModelScale` in the configuration, i.e., `data/hm18.0/cfg/encoder_lowdelay_P_main.cfg`. In addition, you can find the frame QPs in log files. How do `QP`, `IntraQPOffset`, `QPoffset`, `QPOffsetModelOff`, and `QPOffsetModelScale` determine the final frame QPs?

- POC 0 (I-SLICE): `QP + IntraQPOffset`
- POC 1, 2, ... (P-SLICE): `QP + QPoffset + QPOffsetModelOff + QPOffsetModelScale * QP`

For example, the `QP`, `IntraQPOffset`, `QPoffset`, `QPOffsetModelOff`, and `QPOffsetModelScale` are `37`, `-1`, `[5, 4, 5, 4, 5, 4, 5, 1]`, `[-6.5, ..., -6.5, 0]`, and `[0.2590, ..., 0.2590, 0]`, respectively. Then, QP values for POC 0 to 8 are `36`, `45`, `44`, `45`, `44`, `45`, `44`, `45`, and `38`, respectively. Since a lower QP correlates to higher compression quality, POC 0, 2, 4, 6, and 8 are key frames.

> The configuration can vary among different HM versions. In HM 16.5, QP offsets are set to `[5,4,5,1]` (GOPSize: 4).
>
> You can also find a configuration with GOPSize being 4 for HM 18.0 at `data/hm18.0/cfg/misc/encoder_lowdelay_P_main_GOP4.cfg`.

### Use LMDB for faster IO

One can use LMDB to accelerate the IO. Specifically, one can store training patches/images in LMDB files.

Pros:

- Much faster training due to the loading and processing of small patches instead of big images and faster IO of LMDB.
- Lower CPU utility and GPU memory due to the loading and processing of small patches.
- All images (PNG, JPG, etc.) can be stored as PNG.

Cons:

- Higher memory.
- Extra time, computation and storage for LMDB files.
- Once the LMDB file is generated, the cropping manner is also fixed.

Take the DIV2K dataset as an example.

```bash
# cropping and making lmdb for training set

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 --src data/div2k/train\
 --tmp tmp/div2k_lmdb/train\
 --save data/div2k_lmdb/train.lmdb

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 --src data/div2k_lq/bpg/qp37/train\
 --tmp tmp/div2k_lq_lmdb/bpg/qp37/train\
 --save data/div2k_lq_lmdb/bpg/qp37/train.lmdb

# no cropping for test set

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 --no-patch\
 --src data/div2k/valid\
 --save data/div2k_lmdb/valid.lmdb

conda activate powerqe &&\
 python tools/data/prepare_dataset.py\
 --no-patch\
 --src data/div2k_lq/bpg/qp37/valid\
 --save data/div2k_lq_lmdb/bpg/qp37/valid.lmdb
```

For the configuration file with LMDB loading, see `configs/arcnn_div2k_lmdb.py`.

### Use pre-commit hook for code check

PowerQE follows [MMCV](https://github.com/open-mmlab/mmcv/blob/master/CONTRIBUTING.md) and MMEditing to support the pre-commit hook. The configuration file is inherited from `mmediting/.pre-commit-config.yaml`. Installation:

```bash
conda activate powerqe
pip install -U pre-commit
pre-commit install
```

On every commit, linters and formatter will be enforced. You can also run hooks manually:

```bash
pre-commit run --all
```

### Same items between PowerQE and MMEditing

We take the `Compose` in `powerqe/datasets/pipelines/compose.py` as an example.

When constructing the pipeline for a dataset, the dataset (`BaseDataset` in fact) refers to `Compose`. Then, `Compose` refers to `..registry` for `PIPELINES`.

PowerQE has its own transforms such as `PairedCenterCrop`. As a result, PowerQE has to define a new `Compose`, which refers to its own `..registry`.

Note that the `Compose` in PowerQE will not be registered into `PIPELINES`.

[^paper-x265]: *Leveraging Bitstream Metadata for Fast and Accurate Video Compression Correction*, 2022.

[^qp-offset]: Proposal [JCTVC-X0038](http://phenix.it-sudparis.eu/jct/doc_end_user/current_document.php?id=10496).

[^mfqev2]: *MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video*, 2019.
