# Welcome to PowerQE V3

## Main differences to MMEditing

- Support down-sampling before enhancement and up-sampling after enhancement to save memory.
- Save LQ, GT and output when testing.
- Evaluate "LQ vs. GT" and "output vs. GT" when testing.
- Bug fixed.

## Configuration

- bs: batch size
- ps: patch size

| Algorithm | Config       | Memory | Note                             |
| --------- | ------------ | ------ | -------------------------------- |
| SAN       | ps=48, bs=16 | ~17 GB | ps=64? ~44 GB when testing DIV2K |

### Cropping image border before evaluation

Due to the padding of up-sampling, the error at borders is significant. PowerQE follows the common practice in SR to crop image borders before evaluation.

### Test unfolding

It conducts patch-based evaluation to save memory. The accuracy also drops.

Once indicated, it is applied for all kinds of evaluation, including validation and testing.

## Use LMDB for faster IO

One can use LMDB to accelerate the IO. Specifically, one can store training patches/images in LMDB files.

Pros:

- Much faster training due to the loading and processing of small patches instead of big images and faster IO of LMDB.
- Lower CPU utility and GPU memory due to the loading and processing of small patches.
- All images (PNG, JPG, etc.) can be stored as PNG.

Cons:

- Higher memory.
- Extra time, computation and storage for LMDB files.
- Once the LMDB file is generated, the cropping manner is also fixed.

Take the DIV2K dataset as an example.

```bash
# cropping and making lmdb for training set

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-src data/div2k/train/gt \
-tmp tmp/div2k/train/gt_patches \
-save data/div2k/train/gt_patches.lmdb \
-n 16 -ps 128 -step 64

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-src data/div2k/train/lq \
-tmp tmp/div2k/train/lq_patches \
-save data/div2k/train/lq_patches.lmdb \
-n 16 -ps 128 -step 64

# no cropping for test set

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-no-patch \
-src data/div2k/valid/gt \
-save data/div2k/valid/gt.lmdb \
-n 16

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-no-patch \
-src data/div2k/valid/lq \
-save data/div2k/valid/lq.lmdb \
-n 16
```

For the config file with LMDB loading, see `./configs/arcnn_lmdb.py`.

## Use pre-commit hook before code submission

PowerQE follows MMCV[^mmcv] and MMEditing to support the pre-commit hook. The config file is inherited from mmediting/.pre-commit-config.yaml. Installation:

```bash
conda activate powerqe
pip install -U pre-commit
pre-commit install
```

On every commit, linters and formatter will be enforced. You can also run hooks manually:

```bash
pre-commit run --all
```

## Why there are some same items between PowerQE and MMEditing

I take `Compose` as an example.

When constructing the pipelines for a dataset, the dataset (`BaseDataset` in fact) will refer to `Compose`. Then, `Compose` refers to `..registry` for `PIPELINES`.

PowerQE has its pipelines such as `PairedCenterCrop`. As a result, PowerQE has to define a new `Compose`, which refers to its own `..registry`.

Note that `Compose` in PowerQE will not be registered into `PIPELINES`.

[^mmcv]: https://github.com/open-mmlab/mmcv/blob/master/CONTRIBUTING.md
