# Welcome to PowerQE V3

## Create environment

> Chinese users may first update mirrors:
>
> - Conda: https://mirrors.tuna.tsinghua.edu.cn/help/anaconda
> - pip: https://mirrors.tuna.tsinghua.edu.cn/help/pypi

First, create a PowerQE environment:

```bash
conda env create -f environment.yml  # create the powerqe env
conda activate powerqe
```

Then, install MMEditing following mmediting/docs/en/install.md.

```bash
conda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch

pip3 install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.12/index.html
# or
#pip3 install openmim
#mim install mmcv-full==1.7.0

cd mmediting/
pip3 install -e .

# verify
cd ~
python -c "import mmedit; print(mmedit.__version__)"
```

Finally, install requirements of PowerQE:

```bash
cd powerqe/
pip3 install -r requirements.txt
```

## Compress image and video

### BPG

[BPG (better portable graphics)](https://bellard.org/bpg) is a image format based on the intra-frame encoding of the HEVC (high efficiency video coding) standard.

Please refer to the official site and the [GitHub mirror](https://github.com/mirrorer/libbpg/blob/master/README) for instructions. The following is my personal experience of using libbpg v0.9.8.

#### Build

Clone:

```bash
cd data
git clone --depth=1 https://github.com/mirrorer/libbpg.git libbpg
```

Modify libbpg/Makefile:

- Uncomment `USE_JCTVC=y`: We want to use JCTVC.
- Comment `USE_BPGVIEW=y`: We do not need BPGView.

Build:

```bash
cd data/libbpg
make clean
make
```

There may be errors during the build. We need to install the required dependencies according to the error messages. For examples,

```bash
sudo apt-get install libpng-dev
sudo apt-get install libjpeg-dev

# build again
make clean
make
```

#### Example: Compress the DIV2K dataset

> Basic usage:
>
> ```bash
> bpgenc [-q quality] -o bpg-path src-path  # encode .png to .bpg
> bpgdec -o tar-path bpg-path  # decode .bpg to .png
> ```
>
> Check `bpgenc -h` and `bpgdec -h` for detailed usage.

Write a Bash script to run the following script, or paste it directly into the terminal:

```bash
BPGENC_PATH="./data/libbpg/bpgenc"
BPGDEC_PATH="./data/libbpg/bpgdec"
QP=37

# training set
SRC_DIR="./data/div2k/train/gt/"
TMP_DIR="./tmp/data/div2k/train/lq/bpg/qp37/"
TAR_DIR="./data/div2k/train/lq/bpg/qp37/"
mkdir -p $TMP_DIR
mkdir -p $TAR_DIR
for idx in `seq -f '%04g' 1 800`; do
  echo $idx
  $BPGENC_PATH -o $TMP_DIR$idx.bpg -q $QP $SRC_DIR$idx.png
  $BPGDEC_PATH -o $TAR_DIR$idx.png -b 8 $TMP_DIR$idx.bpg
done

# valid set
SRC_DIR="./data/div2k/valid/gt/"
TMP_DIR="./tmp/data/div2k/valid/lq/bpg/qp37/"
TAR_DIR="./data/div2k/valid/lq/bpg/qp37/"
mkdir -p $TMP_DIR
mkdir -p $TAR_DIR
for idx in `seq -f '%04g' 801 900`; do
  echo $idx
  $BPGENC_PATH -o $TMP_DIR$idx.bpg -q $QP $SRC_DIR$idx.png
  $BPGDEC_PATH -o $TAR_DIR$idx.png -b 8 $TMP_DIR$idx.bpg
done
```

To speed up the process, we can also use Python multiprocessing:

```python
import multiprocessing as mp
import os
from tqdm import tqdm


def func_demo(enc_cmd, dec_cmd):
    os.system(enc_cmd)
    os.system(dec_cmd)


if __name__ == '__main__':
    BPGENC_PATH = os.path.abspath('./data/libbpg/bpgenc')
    BPGDEC_PATH = os.path.abspath('./data/libbpg/bpgdec')
    QP = 37
    MAX_NPRO = 20

    # training set

    pool = mp.Pool(processes=MAX_NPRO)

    SRC_DIR = os.path.abspath('./data/div2k/train/gt/')
    TMP_DIR = os.path.abspath('./tmp/data/div2k/train/lq/bpg/qp37/')
    TAR_DIR = os.path.abspath('./data/div2k/train/lq/bpg/qp37/')
    os.makedirs(TMP_DIR)
    os.makedirs(TAR_DIR)

    pbar = tqdm(total=800, ncols=0)
    for idx in range(1, 801):
        src_path = os.path.join(SRC_DIR, f'{idx:04d}.png')
        bpg_path = os.path.join(TMP_DIR, f'{idx:04d}.bpg')
        tar_path = os.path.join(TAR_DIR, f'{idx:04d}.png')
        enc_cmd = f'{BPGENC_PATH} -o {bpg_path} -q {QP} {src_path}'
        dec_cmd = f'{BPGDEC_PATH} -o {tar_path} {bpg_path}'
        pool.apply_async(
            func=func_demo,
            args=(
                enc_cmd,
                dec_cmd,
            ),
            callback=lambda _: pbar.update())
    pool.close()
    pool.join()
    pbar.close()

    # validation set

    pool = mp.Pool(processes=MAX_NPRO)

    SRC_DIR = os.path.abspath('./data/div2k/valid/gt/')
    TMP_DIR = os.path.abspath('./tmp/data/div2k/valid/lq/bpg/qp37/')
    TAR_DIR = os.path.abspath('./data/div2k/valid/lq/bpg/qp37/')
    os.makedirs(TMP_DIR)
    os.makedirs(TAR_DIR)

    pbar = tqdm(total=100, ncols=0)
    for idx in range(801, 901):
        src_path = os.path.join(SRC_DIR, f'{idx:04d}.png')
        bpg_path = os.path.join(TMP_DIR, f'{idx:04d}.bpg')
        tar_path = os.path.join(TAR_DIR, f'{idx:04d}.png')
        enc_cmd = f'{BPGENC_PATH} -o {bpg_path} -q {QP} {src_path}'
        dec_cmd = f'{BPGDEC_PATH} -o {tar_path} {bpg_path}'
        pool.apply_async(
            func=func_demo,
            args=(
                enc_cmd,
                dec_cmd,
            ),
            callback=lambda _: pbar.update())
    pool.close()
    pool.join()
    pbar.close()
```

### HM reference software

[HM](https://vcgit.hhi.fraunhofer.de/jvet/HM) is the reference software for Rec. ITU-T H.265 | ISO/IEC 23008-2 high efficiency video coding (HEVC).

Please refer to the official site and the document (e.g., HM/doc/software-manual.pdf) for instructions. The following is my personal experience of using HM 18.0.

#### Build

```bash
cd data
git clone -b HM-18.0 --depth=1 https://vcgit.hhi.fraunhofer.de/jvet/HM.git hm

cd hm
mkdir build
cd build
cmake .. -DCMAKE_BUILD_TYPE=Release
make -j
```

#### Example: Compress the Vimeo-90K triplet dataset

> Basic usage:
>
> ```bash
> TAppEncoder [--help] [-c cfg.cfg] [--param=value]
> TAppDecoder -b str.bin -o dec.yuv [options]
> ```

#TODO

#### Why we do not use x265

[x265](https://www.x265.org) is a HEVC video encoder application library. Encoding videos using x265 can be much faster than using HM. x265 has been supported by FFmpeg with [libx265](https://trac.ffmpeg.org/wiki/Encode/H.265). As indicated by this paper[^paper-x265], the following script can generate compressed videos that closely resemble the output of HM:

```bash
ffmpeg -video_size <WIDTH>x<HEIGHT> \
    -i <INPUT> \
    -vcodec libx265 \
    -qp <QP> \
    -x265-params <OPTIONS> \
    <OUTPUT>
```

for options:

```bash
<OPTIONS>=
keyint=7:min-keyint=7:no-scenecut:me=full:subme=7:bframes=0:qp=<QP>
```

For research purpose, we need to control the distortion. Therefore, it is better to use a specific configuration of a stable version of the reference software. For example, we may use the [low-delay P](https://vcgit.hhi.fraunhofer.de/jvet/HM/-/blob/fb4486d5ab5d0cd3b6a71659c7d5eb4509f2a4ce/cfg/encoder_lowdelay_P_main.cfg) configuration of HM 18.0 to compress videos. In this configuration, many hyperparameters, including the frame-level QP offset, are set. The above script does not indicate these parameters.

> The QP offset is proposed to achieve better RD performance[^qp-offset]. Some approaches such as MFQEv2[^mfqev2] take advantage of the frame-wise quality fluctuation caused by the QP offset.

## Configuration

### Crop image border before evaluation

Due to the padding of up-sampling, the error at borders is significant. PowerQE follows the common practice in MMEditing to crop image borders before evaluation.

### Test unfolding

When using test unfolding, patch-based evaluation is conducted to save memory. The accuracy may also drop.

## Use LMDB for faster IO

One can use LMDB to accelerate the IO. Specifically, one can store training patches/images in LMDB files.

Pros:

- Much faster training due to the loading and processing of small patches instead of big images and faster IO of LMDB.
- Lower CPU utility and GPU memory due to the loading and processing of small patches.
- All images (PNG, JPG, etc.) can be stored as PNG.

Cons:

- Higher memory.
- Extra time, computation and storage for LMDB files.
- Once the LMDB file is generated, the cropping manner is also fixed.

Take the DIV2K dataset as an example.

```bash
# cropping and making lmdb for training set

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-src data/div2k/train/gt \
-tmp tmp/data/div2k/train/gt_patches \
-save data/div2k/train/gt_patches.lmdb \
-n 16 -ps 128 -step 64

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-src data/div2k/train/lq \
-tmp tmp/data/div2k/train/lq_patches \
-save data/div2k/train/lq_patches.lmdb \
-n 16 -ps 128 -step 64

# no cropping for test set

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-no-patch \
-src data/div2k/valid/gt \
-save data/div2k/valid/gt.lmdb \
-n 16

conda activate powerqe && \
python ./tools/data/prepare_dataset.py \
-no-patch \
-src data/div2k/valid/lq \
-save data/div2k/valid/lq.lmdb \
-n 16
```

For the configuration file with LMDB loading, see `./configs/arcnn_lmdb_div2k.py`.

## Framework

### Use pre-commit hook for auto code check

PowerQE follows [MMCV](https://github.com/open-mmlab/mmcv/blob/master/CONTRIBUTING.md) and MMEditing to support the pre-commit hook. The configuration file is inherited from mmediting/.pre-commit-config.yaml. Installation:

```bash
conda activate powerqe
pip install -U pre-commit
pre-commit install
```

On every commit, linters and formatter will be enforced. You can also run hooks manually:

```bash
pre-commit run --all
```

### Some same items between PowerQE and MMEditing

I take powerqe/datasets/pipelines/compose.py:Compose as an example.

When constructing the pipelines for a dataset, the dataset (BaseDataset in fact) refers to Compose. Then, Compose refers to ..registry for PIPELINES.

PowerQE has its pipelines such as PairedCenterCrop. As a result, PowerQE has to define a new Compose, which refers to its own ..registry.

Note that Compose in PowerQE will not be registered into PIPELINES.

[^paper-x265]: *Leveraging Bitstream Metadata for Fast and Accurate Video Compression Correction*, 2022.

[^qp-offset]: Proposal [JCTVC-X0038](http://phenix.it-sudparis.eu/jct/doc_end_user/current_document.php?id=10496).

[^mfqev2]: *MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video*, 2019.
